from autonomy import HttpServer, Node, NodeDep, Zone

from fastapi import FastAPI
from fastapi.responses import FileResponse, JSONResponse

import asyncio
import secrets
import json


class CodeAnalyzer:
    def files_from_github_repo(self, org, repo, branch="main"):
        import os
        import shutil
        import urllib.request
        import zipfile

        url = f"https://github.com/{org}/{repo}/archive/refs/heads/{branch}.zip"
        zip_path = f"/tmp/{repo}-{branch}.zip"
        urllib.request.urlretrieve(url, zip_path)

        extracted = f"/tmp/{repo}-{branch}"
        if os.path.exists(extracted):
            shutil.rmtree(extracted)

        os.makedirs(extracted)
        with zipfile.ZipFile(zip_path, "r") as zip_ref:
            zip_ref.extractall(extracted)

        os.remove(zip_path)

        files = []
        for root, dirs, file_list in os.walk(extracted):
            for file in file_list:
                full_path = os.path.join(root, file)
                stat = os.stat(full_path)
                if file.endswith(".py") and stat.st_size < 1024 * 50:
                    with open(full_path, "r", encoding="utf-8") as f:
                        content = f.read()
                        relative_path = os.path.relpath(full_path, extracted)
                        relative_path = "./" + relative_path[len(f"{repo}-{branch}/"):]
                        files.append((relative_path, content))

        shutil.rmtree(extracted)
        return files

    async def analyze_file(self, filename, content):
        from autonomy import Agent, Model

        # Configure model with throttling for high-scale batch processing
        # Very conservative settings to prevent OOM and queue exhaustion
        model = Model(
            "nova-micro-v1",
            throttle=True,
            throttle_requests_per_minute=30,           # Conservative start
            throttle_max_requests_in_progress=5,       # Limit concurrent requests
            throttle_max_requests_waiting_in_queue=1000,  # Large queue to absorb bursts
            throttle_max_seconds_to_wait_in_queue=300.0,  # 5 min queue timeout
            throttle_max_retry_attempts=5,
            throttle_initial_seconds_between_retry_attempts=2.0,
            request_timeout=120.0,
        )

        agent = await Agent.start(
            node=self.node,
            instructions="""
                You are an expert in programming with Python and writing secure code.
                When you're given a code snippet you decide if it is secure or not.

                If it is secure output - YES
                If it is not secure output - NO

                Don't say anything else. Only output one upper case word YES or NO.
            """,
            model=model,
            max_execution_time=60.0,
            max_iterations=5,
            timeout=30.0,
        )

        message = f"Filename: {filename}\nContent:\n\n{content}"
        receiver = await agent.send_message(message)
        return (agent, filename, receiver)

    async def collect_results(self, agent, filename, receiver):
        from autonomy import Agent

        try:
            analysis = await receiver.receive_message(timeout=120)
            result = {"file": filename, "analysis": str(analysis[0].content)}
        except Exception as e:
            result = {"file": filename, "analysis": f"ERROR: {str(e)}"}
        finally:
            try:
                await Agent.stop(self.node, agent.name, timeout=10.0)
            except Exception:
                pass  # Best effort cleanup
        return result

    async def analyze_github_repo(self, org, repo, branch="main"):
        from autonomy import gather
        import asyncio

        name = f"{org}/{repo}/{branch}"
        print(name, flush=True)

        try:
            files = await asyncio.to_thread(self.files_from_github_repo, org, repo, branch)

            # Limit concurrency to prevent OOM - only 5 agents at a time
            futures = [self.analyze_file(f, c) for f, c in files]
            agents = await gather(*futures, batch_size=5)

            futures = [self.collect_results(a, f, r) for (a, f, r) in agents]
            analyses = await gather(*futures, batch_size=5)

            return {"repo": name, "number_of_files": len(files), "analysis": analyses}
        except Exception as e:
            return {"repo": name, "error": str(e)}

    async def handle_message(self, context, message):
        import json
        import asyncio

        repos = json.loads(message)

        futures = []
        for repo in repos:
            org_name, repo_name = repo.split("/")
            f = self.analyze_github_repo(org_name, repo_name)
            futures.append(f)
        analyses = await asyncio.gather(*futures)

        reply = json.dumps(analyses)
        await context.reply(reply)


async def run_analyzer(node, repos):
    name = secrets.token_hex(3)
    await node.start_worker(name, CodeAnalyzer())

    repos_json = json.dumps(repos)
    try:
        # Increased timeout for batch processing with throttling
        reply_json = await node.send_and_receive(name, repos_json, timeout=600)
        reply = json.loads(reply_json)
    except Exception as e:
        reply = [{"error": str(e)}]
    finally:
        try:
            await node.stop_worker(name)
        except Exception:
            pass  # Best effort cleanup

    return reply


async def analyze(node, repos):
    runners = await Zone.nodes(node, filter="runner")

    num_runners = len(runners)
    if num_runners == 0:
        return []
    if num_runners > len(repos):
        runners = runners[: len(repos)]
        num_runners = len(runners)

    repos = split_list_into_n_parts(repos, num_runners)
    futures = [run_analyzer(runner, repos[i]) for i, runner in enumerate(runners)]
    replies = await asyncio.gather(*futures)

    return replies


def split_list_into_n_parts(lst, n):
    q, r = divmod(len(lst), n)
    return [lst[i * q + min(i, r): (i + 1) * q + min(i + 1, r)] for i in range(n)]


async def list_workers(node):
    runners = await Zone.nodes(node, filter="runner")
    futures = [runner.list_workers() for runner in runners]
    workers_per_runner = await asyncio.gather(*futures)
    workers = []
    for runner, workers_on_this_runner in zip(runners, workers_per_runner):
        for w in workers_on_this_runner:
            workers.append({"worker_name": w["name"], "runner_name": runner.name})
    return workers


app = FastAPI()


@app.get("/")
async def index():
    return FileResponse("index.html")


@app.post("/analyze")
async def post_analyze(node: NodeDep):
    repos = [
        "pallets/flask",
        "pallets/click",
        "pallets/werkzeug",
        "pallets/jinja",
        "pallets/markupsafe",
        "pallets/itsdangerous",
        "psf/requests",
        "pandas-dev/pandas",
        "simonw/files-to-prompt",
        "simonw/sqlite-utils",
        "pytest-dev/pytest",
        "celery/celery",
        "psf/black",
        "jazzband/pip-tools",
        "python-pillow/Pillow",
        "python-poetry/poetry",
    ]
    response = await analyze(node, repos)
    return JSONResponse(content=response)


@app.get("/runners/workers")
async def get_workers(node: NodeDep):
    workers = await list_workers(node)
    return JSONResponse(content={"workers": workers})


@app.get("/runners")
async def get_runners(node: NodeDep):
    runners = await Zone.nodes(node, filter="runner")
    return JSONResponse(content={"runners": [r.name for r in runners]})


Node.start(http_server=HttpServer(app=app), cache_secure_channels=True)
