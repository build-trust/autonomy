<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Voice Interface - Autonomy Example</title>

    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      body {
        font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
        background: #fffef9;
        min-height: 100vh;
        display: flex;
        align-items: center;
        justify-content: center;
        color: #3d3d3d;
        overflow: hidden;
      }

      .container {
        display: flex;
        flex-direction: row;
        width: 100%;
        height: 100vh;
        padding: 0;
        max-width: none;
      }

      .transcript-panel {
        width: 70%;
        height: 100%;
        display: flex;
        flex-direction: column;
        padding: 40px;
        background: #fffef9;
      }

      .voice-panel {
        width: 30%;
        height: 100%;
        display: flex;
        flex-direction: column;
        align-items: center;
        justify-content: center;
        gap: 40px;
        padding: 40px;
        background: #fffef9;
      }

      .circle-container {
        position: relative;
        width: 200px;
        height: 200px;
      }

      .voice-circle {
        width: 200px;
        height: 200px;
        border-radius: 50%;
        background: linear-gradient(135deg, #e8956c 0%, #d97342 100%);
        cursor: pointer;
        transition: all 0.3s ease;
        display: flex;
        align-items: center;
        justify-content: center;
        box-shadow:
          0 10px 40px rgba(217, 115, 66, 0.25),
          inset 0 2px 10px rgba(255, 255, 255, 0.1);
        position: relative;
        border: 2px solid rgba(232, 149, 108, 0.3);
      }

      .voice-circle:hover {
        transform: scale(1.05);
        box-shadow:
          0 15px 50px rgba(217, 115, 66, 0.35),
          inset 0 2px 10px rgba(255, 255, 255, 0.15);
      }

      .voice-circle.listening {
        animation: pulse-listening 2s ease-in-out infinite;
        background: linear-gradient(135deg, #f0a47a 0%, #e8956c 100%);
      }

      .voice-circle.speaking {
        animation: pulse-speaking 1.5s ease-in-out infinite;
        background: linear-gradient(135deg, #f4a460 0%, #e8956c 100%);
      }

      .voice-circle.processing {
        animation: pulse-processing 1s ease-in-out infinite;
        background: linear-gradient(135deg, #ec9f6e 0%, #e8956c 100%);
      }

      .voice-circle.delegating {
        animation: pulse-delegating 0.8s ease-in-out infinite;
        background: linear-gradient(135deg, #d97342 0%, #c56535 100%);
      }

      .waveform-icon {
        width: 80px;
        height: 80px;
        display: flex;
        align-items: center;
        justify-content: center;
        gap: 6px;
      }

      .waveform-bar {
        width: 8px;
        background: #fffef9;
        border-radius: 4px;
        transition: all 0.3s ease;
      }

      .waveform-bar:nth-child(1) {
        height: 30px;
      }
      .waveform-bar:nth-child(2) {
        height: 50px;
      }
      .waveform-bar:nth-child(3) {
        height: 40px;
      }
      .waveform-bar:nth-child(4) {
        height: 60px;
      }
      .waveform-bar:nth-child(5) {
        height: 35px;
      }

      .voice-circle:hover .waveform-bar {
        background: #ffffff;
      }

      .voice-circle.listening .waveform-bar,
      .voice-circle.speaking .waveform-bar,
      .voice-circle.processing .waveform-bar,
      .voice-circle.delegating .waveform-bar {
        animation: waveform-pulse 1.2s ease-in-out infinite;
      }

      .voice-circle.listening .waveform-bar:nth-child(1) {
        animation-delay: 0s;
      }
      .voice-circle.listening .waveform-bar:nth-child(2) {
        animation-delay: 0.1s;
      }
      .voice-circle.listening .waveform-bar:nth-child(3) {
        animation-delay: 0.2s;
      }
      .voice-circle.listening .waveform-bar:nth-child(4) {
        animation-delay: 0.3s;
      }
      .voice-circle.listening .waveform-bar:nth-child(5) {
        animation-delay: 0.4s;
      }

      .voice-circle.speaking .waveform-bar:nth-child(1) {
        animation-delay: 0.4s;
      }
      .voice-circle.speaking .waveform-bar:nth-child(2) {
        animation-delay: 0.3s;
      }
      .voice-circle.speaking .waveform-bar:nth-child(3) {
        animation-delay: 0.2s;
      }
      .voice-circle.speaking .waveform-bar:nth-child(4) {
        animation-delay: 0.1s;
      }
      .voice-circle.speaking .waveform-bar:nth-child(5) {
        animation-delay: 0s;
      }

      @keyframes waveform-pulse {
        0%,
        100% {
          transform: scaleY(0.6);
          opacity: 0.7;
        }
        50% {
          transform: scaleY(1.2);
          opacity: 1;
        }
      }

      @keyframes pulse-listening {
        0%,
        100% {
          box-shadow:
            0 10px 40px rgba(217, 115, 66, 0.25),
            0 0 30px rgba(232, 149, 108, 0.2),
            inset 0 2px 10px rgba(255, 255, 255, 0.1);
          transform: scale(1);
        }
        50% {
          box-shadow:
            0 10px 60px rgba(217, 115, 66, 0.4),
            0 0 60px rgba(232, 149, 108, 0.35),
            inset 0 2px 15px rgba(255, 255, 255, 0.15);
          transform: scale(1.05);
        }
      }

      @keyframes pulse-speaking {
        0%,
        100% {
          box-shadow:
            0 10px 40px rgba(244, 164, 96, 0.3),
            0 0 35px rgba(244, 164, 96, 0.3),
            inset 0 2px 10px rgba(255, 255, 255, 0.1);
          transform: scale(1);
        }
        50% {
          box-shadow:
            0 10px 60px rgba(244, 164, 96, 0.45),
            0 0 70px rgba(244, 164, 96, 0.45),
            inset 0 2px 15px rgba(255, 255, 255, 0.15);
          transform: scale(1.08);
        }
      }

      @keyframes pulse-processing {
        0%,
        100% {
          box-shadow:
            0 10px 40px rgba(217, 115, 66, 0.25),
            0 0 25px rgba(236, 159, 110, 0.25),
            inset 0 2px 10px rgba(255, 255, 255, 0.1);
          transform: scale(1);
        }
        50% {
          box-shadow:
            0 10px 60px rgba(217, 115, 66, 0.35),
            0 0 50px rgba(236, 159, 110, 0.35),
            inset 0 2px 15px rgba(255, 255, 255, 0.15);
          transform: scale(1.03);
        }
      }

      @keyframes pulse-delegating {
        0%,
        100% {
          box-shadow:
            0 10px 40px rgba(197, 101, 53, 0.3),
            0 0 30px rgba(217, 115, 66, 0.25),
            inset 0 2px 10px rgba(255, 255, 255, 0.1);
          transform: scale(1);
        }
        50% {
          box-shadow:
            0 10px 60px rgba(197, 101, 53, 0.45),
            0 0 60px rgba(217, 115, 66, 0.4),
            inset 0 2px 15px rgba(255, 255, 255, 0.15);
          transform: scale(1.06);
        }
      }

      .audio-wave {
        position: absolute;
        top: 50%;
        left: 50%;
        transform: translate(-50%, -50%);
        width: 240px;
        height: 240px;
        border-radius: 50%;
        border: 2px solid rgba(232, 149, 108, 0.25);
        opacity: 0;
        animation: wave-expand 2s ease-out infinite;
        pointer-events: none;
      }

      .audio-wave:nth-child(2) {
        animation-delay: 0.5s;
      }
      .audio-wave:nth-child(3) {
        animation-delay: 1s;
      }

      .voice-circle.listening ~ .audio-wave,
      .voice-circle.speaking ~ .audio-wave {
        opacity: 1;
      }

      @keyframes wave-expand {
        0% {
          width: 200px;
          height: 200px;
          opacity: 0.8;
        }
        100% {
          width: 300px;
          height: 300px;
          opacity: 0;
        }
      }

      .status-text {
        font-size: 18px;
        color: #d97342;
        text-align: center;
        min-height: 30px;
        transition: all 0.3s ease;
        font-weight: 500;
        letter-spacing: 0.5px;
      }

      .status-text.active {
        color: #c56535;
      }

      .transcript-container {
        width: 100%;
        flex: 1;
        overflow-y: auto;
        background: rgba(232, 149, 108, 0.05);
        border: 1px solid rgba(232, 149, 108, 0.2);
        border-radius: 12px;
        padding: 24px;
      }

      .transcript-item {
        margin-bottom: 12px;
        padding: 8px 12px;
        border-radius: 8px;
        font-size: 14px;
        line-height: 1.5;
      }

      .transcript-item.user {
        background: rgba(232, 149, 108, 0.15);
        margin-left: 20px;
      }

      .transcript-item.assistant {
        background: rgba(217, 115, 66, 0.15);
        margin-right: 20px;
      }

      .transcript-item .role {
        font-size: 11px;
        text-transform: uppercase;
        letter-spacing: 1px;
        color: #8b7355;
        margin-bottom: 4px;
      }

      .controls {
        display: flex;
        gap: 16px;
        margin-top: 20px;
      }

      .control-button {
        padding: 12px 32px;
        background: rgba(232, 149, 108, 0.1);
        border: 2px solid rgba(232, 149, 108, 0.3);
        border-radius: 24px;
        color: #d97342;
        font-size: 16px;
        cursor: pointer;
        transition: all 0.3s ease;
        font-weight: 500;
        letter-spacing: 0.5px;
      }

      .control-button:hover {
        background: rgba(244, 164, 96, 0.2);
        border-color: rgba(244, 164, 96, 0.6);
        transform: translateY(-2px);
        box-shadow: 0 5px 20px rgba(217, 115, 66, 0.25);
      }

      .control-button.danger {
        border-color: rgba(197, 101, 53, 0.4);
        color: #c56535;
      }

      .control-button.danger:hover {
        background: rgba(197, 101, 53, 0.2);
        border-color: rgba(197, 101, 53, 0.6);
      }

      .connection-status {
        position: fixed;
        top: 20px;
        right: 20px;
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 12px;
        background: rgba(255, 254, 249, 0.9);
        border: 1px solid rgba(232, 149, 108, 0.3);
        font-weight: 500;
      }

      .connection-status.connected {
        border-color: rgba(217, 115, 66, 0.5);
        color: #d97342;
      }

      .connection-status.disconnected {
        border-color: rgba(197, 101, 53, 0.5);
        color: #c56535;
      }
    </style>
  </head>
  <body>
    <div id="connectionStatus" class="connection-status disconnected">Disconnected</div>

    <div class="container">
      <div class="transcript-panel">
        <div id="transcriptContainer" class="transcript-container">
          <div style="color: #8b7355; font-size: 13px; text-align: center">Conversation will appear here...</div>
        </div>
      </div>

      <div class="voice-panel">
        <div class="circle-container">
          <button id="voiceCircle" class="voice-circle">
            <div class="waveform-icon">
              <div class="waveform-bar"></div>
              <div class="waveform-bar"></div>
              <div class="waveform-bar"></div>
              <div class="waveform-bar"></div>
              <div class="waveform-bar"></div>
            </div>
          </button>
          <div class="audio-wave"></div>
          <div class="audio-wave"></div>
          <div class="audio-wave"></div>
        </div>

        <div id="status" class="status-text">Click to start</div>

        <div class="controls">
          <button id="endButton" class="control-button danger" style="display: none">End Session</button>
        </div>
      </div>
    </div>

    <script>
      // State
      let ws = null;
      let mediaStream = null;
      let audioContext = null;
      let isRecording = false;
      let isConnected = false;

      // Audio playback
      let playbackAudioContext = null;
      let nextPlayTime = 0;
      let scheduledSources = []; // Track audio sources for interruption handling
      let isSpeaking = false; // Track if assistant is currently speaking

      // DOM elements
      const voiceCircle = document.getElementById("voiceCircle");
      const status = document.getElementById("status");
      const endButton = document.getElementById("endButton");
      const connectionStatus = document.getElementById("connectionStatus");
      const transcriptContainer = document.getElementById("transcriptContainer");

      // Connect to WebSocket
      async function connect() {
        const protocol = window.location.protocol === "https:" ? "wss:" : "ws:";
        const wsUrl = `${protocol}//${window.location.host}/agents/interviewer/voice`;

        try {
          updateStatus("Connecting...");
          console.log("ðŸ”Œ Connecting to:", wsUrl);

          ws = new WebSocket(wsUrl);

          ws.onopen = () => {
            console.log("âœ… WebSocket connected");
            isConnected = true;
            connectionStatus.textContent = "Connected";
            connectionStatus.className = "connection-status connected";
            updateStatus("Connected! Click to start talking");

            // Send initial config
            ws.send(JSON.stringify({ type: "config" }));
          };

          ws.onmessage = async (event) => {
            try {
              const data = JSON.parse(event.data);
              await handleServerMessage(data);
            } catch (err) {
              console.error("âŒ Error handling message:", err);
            }
          };

          ws.onerror = (error) => {
            console.error("âŒ WebSocket error:", error);
            updateStatus("Connection error");
          };

          ws.onclose = (event) => {
            console.log("ðŸ“ª WebSocket closed:", event.code, event.reason);
            isConnected = false;
            connectionStatus.textContent = "Disconnected";
            connectionStatus.className = "connection-status disconnected";
            if (isRecording) {
              stopRecording();
            }
            updateStatus("Disconnected. Refresh to reconnect.");
          };
        } catch (error) {
          console.error("âŒ Connection error:", error);
          updateStatus("Failed to connect");
        }
      }

      // Handle messages from server
      async function handleServerMessage(data) {
        const eventType = data.type;

        switch (eventType) {
          case "connected":
            console.log("âœ… Server confirmed:", data.message);
            if (data.config) {
              console.log("   Config:", data.config);
            }
            break;

          case "audio":
            // Play audio from realtime API
            isSpeaking = true;
            await playAudioChunk(data.audio);
            setCircleState("speaking");
            updateStatus("Speaking...");
            break;

          case "transcript":
            // Complete transcript (user or assistant)
            if (data.role === "user") {
              console.log("ðŸ—£ï¸ User:", data.text);
              addTranscript("user", data.text);
            } else {
              console.log("ðŸ¤– Assistant:", data.text);
              addTranscript("assistant", data.text);
            }
            break;

          case "transcript_delta":
            // Streaming delta (optional - for real-time display if needed)
            // Currently we just wait for the complete transcript
            break;

          case "speech_started":
            // Always clear audio on speech start for responsive interruption
            console.log("ðŸŽ¤ Speech started - clearing any playing audio");
            clearAudioQueue();
            isSpeaking = false;
            setCircleState("listening");
            updateStatus("Listening...");
            break;

          case "speech_stopped":
            console.log("â¸ï¸ Speech stopped");
            setCircleState("processing");
            updateStatus("Processing...");
            break;

          case "response_complete":
            console.log("âœ… Response complete");
            isSpeaking = false;
            setCircleState("listening");
            updateStatus("Listening...");
            break;

          case "error":
            console.error("âŒ Server error:", data.error);
            updateStatus("Error: " + data.error);
            break;
        }
      }

      // Start recording
      async function startRecording() {
        if (!isConnected) {
          // Try to reconnect if not connected
          updateStatus("Reconnecting...");
          await connect();
          // Wait a bit for connection to establish
          await new Promise((resolve) => setTimeout(resolve, 500));
          if (!isConnected) {
            updateStatus("Failed to connect. Please try again.");
            return;
          }
        }

        if (isRecording) {
          stopRecording();
          return;
        }

        try {
          mediaStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              channelCount: 1,
              sampleRate: 24000,
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
          });

          audioContext = new (window.AudioContext || window.webkitAudioContext)({
            sampleRate: 24000,
          });

          const source = audioContext.createMediaStreamSource(mediaStream);
          const processor = audioContext.createScriptProcessor(4096, 1, 1);

          processor.onaudioprocess = (e) => {
            if (!isRecording || !ws || ws.readyState !== WebSocket.OPEN) return;

            const inputData = e.inputBuffer.getChannelData(0);

            // Client-side interruption detection: check audio level
            // If user is speaking loudly while assistant is speaking, interrupt immediately
            if (isSpeaking && scheduledSources.length > 0) {
              let maxLevel = 0;
              for (let i = 0; i < inputData.length; i++) {
                const absLevel = Math.abs(inputData[i]);
                if (absLevel > maxLevel) maxLevel = absLevel;
              }
              // Threshold of 0.02 filters out background noise but catches speech
              if (maxLevel > 0.02) {
                console.log("âš¡ Client-side interruption detected (level:", maxLevel.toFixed(3), ")");
                clearAudioQueue();
                isSpeaking = false;
                setCircleState("listening");
                updateStatus("Listening...");
              }
            }

            const pcm16 = new Int16Array(inputData.length);

            for (let i = 0; i < inputData.length; i++) {
              const s = Math.max(-1, Math.min(1, inputData[i]));
              pcm16[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
            }

            const audioBase64 = btoa(String.fromCharCode(...new Uint8Array(pcm16.buffer)));
            ws.send(JSON.stringify({ type: "audio", audio: audioBase64 }));
          };

          source.connect(processor);
          processor.connect(audioContext.destination);

          isRecording = true;
          setCircleState("listening");
          updateStatus("Listening...");
          endButton.style.display = "block";

          // Clear old transcripts
          transcriptContainer.innerHTML =
            '<div style="color: #8b7355; font-size: 13px; text-align: center">Conversation started...</div>';

          console.log("ðŸŽ¤ Recording started");
        } catch (error) {
          console.error("âŒ Error starting recording:", error);
          updateStatus("Microphone access denied");
        }
      }

      // Stop recording
      function stopRecording() {
        if (!isRecording) return;

        console.log("â¹ï¸ Stopping recording");
        isRecording = false;

        if (mediaStream) {
          mediaStream.getTracks().forEach((track) => track.stop());
          mediaStream = null;
        }

        if (audioContext) {
          audioContext.close();
          audioContext = null;
        }

        setCircleState("");
        updateStatus("Stopped");
        endButton.style.display = "none";

        if (ws && ws.readyState === WebSocket.OPEN) {
          ws.send(JSON.stringify({ type: "close" }));
        }
      }

      // Clear audio queue - stops all scheduled audio immediately (for interruption handling)
      function clearAudioQueue() {
        if (scheduledSources.length === 0) return; // Nothing to clear

        console.log("ðŸ”‡ Clearing audio queue (" + scheduledSources.length + " sources)");

        // Stop all scheduled audio sources
        scheduledSources.forEach((source) => {
          try {
            source.stop();
          } catch (e) {
            // Source may have already finished playing
          }
        });
        scheduledSources = [];

        // Reset playback timing to allow immediate new playback
        if (playbackAudioContext) {
          nextPlayTime = playbackAudioContext.currentTime;
        }
      }

      // Play audio chunk
      async function playAudioChunk(base64Audio) {
        try {
          if (!playbackAudioContext) {
            playbackAudioContext = new (window.AudioContext || window.webkitAudioContext)({
              sampleRate: 24000,
            });
            nextPlayTime = playbackAudioContext.currentTime;
          }

          const audioBytes = Uint8Array.from(atob(base64Audio), (c) => c.charCodeAt(0));
          const pcm16 = new Int16Array(audioBytes.buffer);
          const float32 = new Float32Array(pcm16.length);

          for (let i = 0; i < pcm16.length; i++) {
            float32[i] = pcm16[i] / 32768.0;
          }

          const audioBuffer = playbackAudioContext.createBuffer(1, float32.length, 24000);
          audioBuffer.getChannelData(0).set(float32);

          const source = playbackAudioContext.createBufferSource();
          source.buffer = audioBuffer;
          source.connect(playbackAudioContext.destination);

          // Track this source for potential interruption
          source.onended = () => {
            scheduledSources = scheduledSources.filter((s) => s !== source);
          };
          scheduledSources.push(source);

          if (nextPlayTime < playbackAudioContext.currentTime) {
            nextPlayTime = playbackAudioContext.currentTime;
          }
          source.start(nextPlayTime);
          nextPlayTime += audioBuffer.duration;
        } catch (error) {
          console.error("âŒ Error playing audio:", error);
        }
      }

      // Set circle visual state
      function setCircleState(state) {
        voiceCircle.classList.remove("listening", "speaking", "processing", "delegating");
        if (state) {
          voiceCircle.classList.add(state);
        }
      }

      // Update status text
      function updateStatus(message) {
        status.textContent = message;
        status.className = "status-text" + (message.includes("...") ? " active" : "");
      }

      // Add transcript to UI
      function addTranscript(role, text) {
        // Remove placeholder if present
        const placeholder = transcriptContainer.querySelector('div[style*="color: #8b7355"]');
        if (placeholder) {
          placeholder.remove();
        }

        const item = document.createElement("div");
        item.className = "transcript-item " + role;
        item.innerHTML = `
          <div class="role">${role === "user" ? "You" : "Assistant"}</div>
          <div>${text}</div>
        `;
        transcriptContainer.appendChild(item);
        transcriptContainer.scrollTop = transcriptContainer.scrollHeight;
      }

      // End session
      function endSession() {
        stopRecording();
        clearAudioQueue(); // Stop any playing audio
        scheduledSources = []; // Reset tracking array
        isSpeaking = false;
        if (ws) {
          ws.close();
          ws = null;
        }
        if (playbackAudioContext) {
          playbackAudioContext.close();
          playbackAudioContext = null;
        }
        updateStatus("Session ended. Click to start again.");
      }

      // Event listeners
      voiceCircle.addEventListener("click", startRecording);
      endButton.addEventListener("click", endSession);

      // Connect on load
      window.addEventListener("load", connect);

      // Cleanup on unload
      window.addEventListener("beforeunload", () => {
        stopRecording();
        if (ws) ws.close();
      });
    </script>
  </body>
</html>
